{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "# from pyspark.sql import SparkSession\n",
    "# from graphframes import GraphFrame\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../\")  # Add \"../\" to utils folder path\n",
    "from utils import globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME_GL = globals.DATA_PATH + 'output_2_2.txt'\n",
    "FILENAME_CORPUS = globals.DATA_PATH + 'output_1_3.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICTIONARY_PATH = globals.DATA_PATH + \"dictionary.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_SPECIES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['438']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test line cleaning method\n",
    "with open(FILENAME_GL) as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "a = content[0]\n",
    "\n",
    "re.sub('[null\\t\\n\\[\\]\\\"\"]', '', a).replace(' ', '').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[438], [1251], [119], [937], [881]]\n"
     ]
    }
   ],
   "source": [
    "GL = []\n",
    "\n",
    "with open(FILENAME_GL) as f:\n",
    "    content_vertices = f.readlines()\n",
    "\n",
    "for line in content_vertices:\n",
    "    clean_line = re.sub('[null\\t\\n\\[\\]\\\"\"]', '', line).replace(' ', '').split(',')\n",
    "    GL.append(list(map(int, clean_line))) # Convert all strings in a list to int\n",
    "    \n",
    "print(GL[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dictionary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary.load(DICTIONARY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 21],\n",
       "  [1, 4],\n",
       "  [2, 5],\n",
       "  [3, 18],\n",
       "  [4, 4],\n",
       "  [5, 4],\n",
       "  [6, 1],\n",
       "  [7, 10],\n",
       "  [8, 2],\n",
       "  [9, 2],\n",
       "  [10, 3],\n",
       "  [11, 1],\n",
       "  [12, 18],\n",
       "  [13, 2],\n",
       "  [14, 4],\n",
       "  [15, 14],\n",
       "  [16, 3],\n",
       "  [18, 1],\n",
       "  [19, 6],\n",
       "  [20, 5],\n",
       "  [21, 3],\n",
       "  [23, 5],\n",
       "  [24, 2],\n",
       "  [27, 1],\n",
       "  [28, 3],\n",
       "  [29, 3],\n",
       "  [30, 3],\n",
       "  [31, 1],\n",
       "  [32, 1],\n",
       "  [34, 2],\n",
       "  [35, 3],\n",
       "  [36, 1],\n",
       "  [37, 1],\n",
       "  [38, 5],\n",
       "  [41, 1],\n",
       "  [42, 4],\n",
       "  [43, 2],\n",
       "  [44, 3],\n",
       "  [45, 19],\n",
       "  [46, 4],\n",
       "  [47, 4],\n",
       "  [48, 5],\n",
       "  [49, 3],\n",
       "  [50, 4],\n",
       "  [52, 3],\n",
       "  [54, 2],\n",
       "  [55, 6],\n",
       "  [56, 5],\n",
       "  [57, 3],\n",
       "  [58, 8],\n",
       "  [59, 2],\n",
       "  [61, 4],\n",
       "  [62, 3],\n",
       "  [63, 1],\n",
       "  [64, 1],\n",
       "  [65, 3],\n",
       "  [67, 5],\n",
       "  [68, 6],\n",
       "  [69, 1],\n",
       "  [70, 5],\n",
       "  [71, 5],\n",
       "  [72, 2],\n",
       "  [73, 3],\n",
       "  [74, 5],\n",
       "  [76, 1],\n",
       "  [79, 2],\n",
       "  [80, 4],\n",
       "  [81, 1],\n",
       "  [82, 1],\n",
       "  [91, 7],\n",
       "  [92, 3],\n",
       "  [94, 3],\n",
       "  [95, 4],\n",
       "  [97, 4],\n",
       "  [98, 3],\n",
       "  [99, 14],\n",
       "  [100, 2],\n",
       "  [101, 1],\n",
       "  [103, 2],\n",
       "  [105, 2],\n",
       "  [106, 1],\n",
       "  [107, 1],\n",
       "  [110, 1],\n",
       "  [114, 2],\n",
       "  [116, 1],\n",
       "  [117, 1],\n",
       "  [120, 1],\n",
       "  [121, 2],\n",
       "  [124, 2],\n",
       "  [125, 3],\n",
       "  [126, 16],\n",
       "  [127, 2],\n",
       "  [128, 1],\n",
       "  [129, 8],\n",
       "  [130, 6],\n",
       "  [131, 8],\n",
       "  [133, 2],\n",
       "  [134, 1],\n",
       "  [135, 18],\n",
       "  [136, 6],\n",
       "  [137, 2],\n",
       "  [138, 1],\n",
       "  [139, 7],\n",
       "  [140, 2],\n",
       "  [141, 18],\n",
       "  [142, 4],\n",
       "  [143, 4],\n",
       "  [144, 7],\n",
       "  [145, 5],\n",
       "  [146, 4],\n",
       "  [147, 2],\n",
       "  [148, 8],\n",
       "  [149, 1],\n",
       "  [150, 3],\n",
       "  [151, 3],\n",
       "  [152, 9],\n",
       "  [153, 3],\n",
       "  [154, 4],\n",
       "  [155, 6],\n",
       "  [156, 12],\n",
       "  [157, 1],\n",
       "  [158, 2],\n",
       "  [159, 2],\n",
       "  [160, 6],\n",
       "  [161, 2],\n",
       "  [162, 3],\n",
       "  [163, 1],\n",
       "  [164, 1],\n",
       "  [165, 1],\n",
       "  [166, 1],\n",
       "  [167, 5],\n",
       "  [168, 2],\n",
       "  [169, 1],\n",
       "  [170, 1],\n",
       "  [171, 1],\n",
       "  [172, 3],\n",
       "  [173, 3],\n",
       "  [174, 2],\n",
       "  [175, 2],\n",
       "  [176, 3],\n",
       "  [177, 2],\n",
       "  [178, 3],\n",
       "  [179, 1],\n",
       "  [180, 5],\n",
       "  [181, 12],\n",
       "  [182, 2],\n",
       "  [183, 16],\n",
       "  [184, 4],\n",
       "  [185, 1],\n",
       "  [186, 3],\n",
       "  [187, 3],\n",
       "  [188, 5],\n",
       "  [189, 5],\n",
       "  [190, 5],\n",
       "  [191, 3],\n",
       "  [192, 3],\n",
       "  [193, 5],\n",
       "  [194, 4],\n",
       "  [195, 1],\n",
       "  [196, 6],\n",
       "  [197, 1],\n",
       "  [198, 6],\n",
       "  [199, 8],\n",
       "  [200, 5],\n",
       "  [201, 16],\n",
       "  [202, 1],\n",
       "  [203, 2],\n",
       "  [204, 5],\n",
       "  [205, 1],\n",
       "  [206, 3],\n",
       "  [207, 3],\n",
       "  [208, 2],\n",
       "  [209, 2],\n",
       "  [210, 2],\n",
       "  [211, 2],\n",
       "  [212, 1],\n",
       "  [213, 5],\n",
       "  [214, 3],\n",
       "  [215, 2],\n",
       "  [216, 8],\n",
       "  [217, 11],\n",
       "  [218, 9],\n",
       "  [219, 1],\n",
       "  [220, 16],\n",
       "  [221, 5],\n",
       "  [222, 1],\n",
       "  [223, 2],\n",
       "  [224, 2],\n",
       "  [225, 19],\n",
       "  [226, 15],\n",
       "  [227, 1],\n",
       "  [228, 12]]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "with open(FILENAME_CORPUS) as f:\n",
    "    content_corpus = f.readlines()\n",
    "\n",
    "for line in content_corpus:\n",
    "    clean_line = json.loads(line.replace('null\\t', '{\"a\":').replace(\"\\n\", \"}\"))[\"a\"][1]\n",
    "    corpus.append(clean_line)\n",
    "\n",
    "corpus[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1281,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dist(dist, groups, seeds, only_seed=True):\n",
    "    res = []\n",
    "    if only_seed:\n",
    "        print(seeds)\n",
    "        for seednodes in seeds:\n",
    "            tmp = dist[seednodes, :]\n",
    "            print(tmp)\n",
    "            if globals.GROUP_AGGREGATION == \"MEAN\":\n",
    "                res += [np.mean(tmp, axis=0)]\n",
    "            elif globals.GROUP_AGGREGATION == \"MEDIAN\":\n",
    "                res += [np.median(tmp, axis=0)]\n",
    "    else:\n",
    "        for groupnodes in groups:\n",
    "            tmp = dist[groupnodes, :]\n",
    "            if globals.GROUP_AGGREGATION == \"MEAN\":\n",
    "                res += [np.mean(tmp, axis=0)]\n",
    "\n",
    "            elif globals.GROUP_AGGREGATION == \"MEDIAN\":\n",
    "                res += [np.median(tmp, axis=0)]\n",
    "                \n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_groups( group_dist ):\n",
    "    if globals.SCALING:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(group_dist)\n",
    "        print(X_scaled)\n",
    "    else:\n",
    "        X_scaled = group_dist\n",
    "\n",
    "    if globals.CLUSTERING_METHOD == 'KMEANS':\n",
    "        # clustering by k-means\n",
    "        kmeans = KMeans( n_clusters=NUM_OF_SPECIES, init='k-means++')\n",
    "        y_grp_cl = kmeans.fit_predict( X_scaled )\n",
    "        \n",
    "    elif globals.CLUSTERING_METHOD == 'SPECTRAL':\n",
    "        spectral = SpectralClustering(n_clusters=NUM_OF_SPECIES, eigen_solver='arpack',\n",
    "                                      affinity=\"nearest_neighbors\")\n",
    "        #spectral = SpectralClustering(n_clusters=NUM_OF_SPECIES, eigen_solver='arpack',\n",
    "        #                              affinity=\"rbf\")\n",
    "        y_grp_cl = spectral.fit_predict( X_scaled )\n",
    "\n",
    "    return y_grp_cl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run command for `compute_dist()`\n",
    "```python\n",
    "corpus_m = gensim.matutils.corpus2dense(corpus, len(dictionary.keys())).T\n",
    "kmer_group_dist = compute_dist( corpus_m, GL, SL, only_seed=False )\n",
    "z_kmer_grp_cl = cluster_groups( kmer_group_dist )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_m = gensim.matutils.corpus2dense(corpus, len(dictionary.keys())).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL = []\n",
    "\n",
    "kmer_group_dist = compute_dist(corpus_m, GL, SL, only_seed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24. ,  9. , 10. , ...,  0. ,  0. ,  0. ],\n",
       "       [29. ,  8. ,  6. , ...,  0. ,  0. ,  0. ],\n",
       "       [ 6. ,  2. ,  4. , ...,  0. ,  0. ,  0. ],\n",
       "       ...,\n",
       "       [36. ,  7.5, 13. , ...,  0. ,  0. ,  0. ],\n",
       "       [ 7. ,  3. ,  5. , ...,  0. ,  0. ,  0. ],\n",
       "       [33. , 10. ,  9. , ...,  0. ,  0. ,  0. ]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmer_group_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1101, 263)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(kmer_group_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.49923995  1.2308713   0.6810736  ... -0.03015113 -0.03015113\n",
      "  -0.03015113]\n",
      " [ 0.9406396   0.8763093  -0.27496645 ... -0.03015113 -0.03015113\n",
      "  -0.03015113]\n",
      " [-1.0897987  -1.251063   -0.7529865  ... -0.03015113 -0.03015113\n",
      "  -0.03015113]\n",
      " ...\n",
      " [ 1.558599    0.69902825  1.3981037  ... -0.03015113 -0.03015113\n",
      "  -0.03015113]\n",
      " [-1.0015187  -0.89650095 -0.51397645 ... -0.03015113 -0.03015113\n",
      "  -0.03015113]\n",
      " [ 1.2937593   1.5854332   0.4420636  ... -0.03015113 -0.03015113\n",
      "  -0.03015113]]\n"
     ]
    }
   ],
   "source": [
    "z_kmer_grp_cl = cluster_groups(kmer_group_dist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
