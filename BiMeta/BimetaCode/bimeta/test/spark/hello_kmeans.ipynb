{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0 - Create example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n",
      "|other| lat|  long|\n",
      "+-----+----+------+\n",
      "|    0|33.3| -17.5|\n",
      "|    1|40.4| -20.5|\n",
      "|    2|28.0| -23.9|\n",
      "|    3|29.5| -19.0|\n",
      "|    4|32.8|-18.84|\n",
      "+-----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[0, 33.3, -17.5],\n",
    "                              [1, 40.4, -20.5],\n",
    "                              [2, 28., -23.9],\n",
    "                              [3, 29.5, -19.0],\n",
    "                              [4, 32.8, -18.84]\n",
    "                             ],\n",
    "                              [\"other\",\"lat\", \"long\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Assemble your features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to most ML packages out there, Spark ML requires your input features to be gathered in a ***single column*** of your dataframe, usually named `features`; and it provides a specific method for doing this, `VectorAssembler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+-------------+\n",
      "|other| lat|  long|     features|\n",
      "+-----+----+------+-------------+\n",
      "|    0|33.3| -17.5| [33.3,-17.5]|\n",
      "|    1|40.4| -20.5| [40.4,-20.5]|\n",
      "|    2|28.0| -23.9| [28.0,-23.9]|\n",
      "|    3|29.5| -19.0| [29.5,-19.0]|\n",
      "|    4|32.8|-18.84|[32.8,-18.84]|\n",
      "+-----+----+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=[\"lat\", \"long\"], outputCol=\"features\")\n",
    "new_df = vecAssembler.transform(df)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As perhaps already guessed, the argument `inputCols` serves to tell `VectorAssembler` which particular columns in our dataframe are to be used as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Fit your KMeans model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(k=2, seed=1)  # 2 clusters here\n",
    "model = kmeans.fit(new_df.select('features'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`select('features')` here serves to tell the algorithm which column of the dataframe to use for clustering - remember that, after Step 1 above, your original `lat` & `long` features are no more directly used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Transform your initial dataframe to include cluster assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+-------------+----------+\n",
      "|other| lat|  long|     features|prediction|\n",
      "+-----+----+------+-------------+----------+\n",
      "|    0|33.3| -17.5| [33.3,-17.5]|         0|\n",
      "|    1|40.4| -20.5| [40.4,-20.5]|         0|\n",
      "|    2|28.0| -23.9| [28.0,-23.9]|         1|\n",
      "|    3|29.5| -19.0| [29.5,-19.0]|         0|\n",
      "|    4|32.8|-18.84|[32.8,-18.84]|         0|\n",
      "+-----+----+------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed = model.transform(new_df)\n",
    "transformed.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last column of the `transformed` dataframe, `prediction`, shows the cluster assignment - in my example case, I have ended up with 4 records in cluster #0 and 1 record in cluster #1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource:\n",
    "- [KMeans clustering in PySpark](https://stackoverflow.com/a/47593712/9500955)\n",
    "- [PySpark ML: Get KMeans cluster statistics](https://stackoverflow.com/a/47156822/9500955)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
